= Memory Leak Investigation

This document shall focus on investigating memory leaking.

We shall start with https://github.com/xtdb/xtdb/issues/4757 - current running theory is that this relates to cancelled queries leaving behind some state in netty - presumably we need to be using DIRECT memory at the time, so need to force that in some way.

* Large JOIN queries seem to be the most likely culprit for direct memory usage - so we should try to reproduce with a large JOIN query, and then cancel it.

== Setup / Method

For this, will setup:

* Setting up a REPL with:
** A limited amount of JVM - `JDK_JAVA_OPTIONS="-Xmx3000m -Xms3000m -XX:MaxDirectMemorySize=3000m -XX:MaxMetaspaceSize=500m"`
** A number of memory leak tracking opts: `-XX:+UnlockDiagnosticVMOptions -XX:+PrintNMTStatistics -XX:NativeMemoryTracking=detail -Darrow.memory.debug.allocator=true -Dio.netty.leakDetection.level=paranoid`
* Run a memory only node - don't want anything pulled into memory cache to confuse things in terms of tracking netty memory usage.
** Added the following config to dev.clj
+
[src, clojure]
====
(def debug-config
  {::xtdb {:node-opts {:server {:port 5432
                                :host "*"}
                       :healthz {:port 8080
                                 :host "*"}}}})
(ir/set-prep! (fn [] debug-config))
====
* Setup prometheus & grafana from `monitoring` to track metrics - primarily Netty memory usage.

Then, to attempt to replicate the issue:

* Connect to the node with psql.
** `psql -h 0.0.0.0 -p 5432 -d xtdb`
* Run a query that should have a large direct memory usage, ie, a large LEFT JOIN query:
+
[src, sql]
====
SELECT * FROM GENERATE_SERIES(1, 10000000) AS l(n) LEFT JOIN GENERATE_SERIES(1, 10000000) AS r(m) ON n = m ORDER BY n LIMIT 100;
====
** Due to left join build ordering, should be quick on the heap memory side but should need to allocate a lot of direct memory for the output.
* I can then cancel the query from psql with Ctrl-C, if I want to.

== Initial observations

Initially, I just wanted to see how the direct memory usage looks over time as I run the above query to see if I can spot anything odd. I would expect:

* Some amount of direct memory usage as the query runs - this should be reflected in the netty memory usage metric.
* Following the query completing, the netty memory usage should drop back down to near zero, as the memory is released.

So, running the large generate series WITHOUT cancelling, I see the following (this is the results of a few different simultaneous runs):

image::normalrun.png[]

As we can see:

* Netty memory usage spikes to around 500MB during the query, which seems reasonable.
* It will remain at that value for a while as I fetch the results, but then it drops back down to near zero as expected.

All good so far - now, if we cancel a few of those queries (ie, I use CTRL+C on pgwire), we see the following (a side by side view over netty allocated memories and cancelled connection count):

image::cancelled-queries.png[]

Interestingly: 

* Netty memory usage does 'top off' at different points, depending on when I cancel the query.
* The usage does drop back down to near zero afterwards, which would suggest that there isn't a leak in this case.

To try something a bit more drastic, rather than cancelling the queries with CTRL+C, I instead just kill the psql process mid query. This results in the following:

image::killed-queries.png[]

So:

* Cancelled connections is, interestingly, unchanged.
* Again, netty memory usage goes up to around 500MB, and then drops back down to near zero.

However, looking a bit closer at values following one of these kills:

image::leftover-netty.png[]

Hard to see from the graph, but:

* Prior to the query, netty used memory sat at 8.4MB (this was true for all of the graphs above - not sure what I may have initially to have prompted that).
* AFTER the query - netty used memory sits at 12.6MB.

Now that isn't particularly drastic, but it does suggest that there may be some small amount of memory that isn't being released following a killed query, in certain cases.

* This didn't happen for BOTH of the killed queries I ran, only when I killed the query very quickly.
* Presumably will relate somewhat to "when" the thing was cancelled.

Following this, here's what happens when I run the query and send the cancel signal IMMEDIATELY after starting the query, a few times in quick succession:

image::quickly-cancelled.png[]

* After the first of these, my "base" value became 16.8MB.
* The following queries didn't share the same behaviour.

Still, it is interesting to see that the netty memory usage can increase in this way, even if it is only a small amount. ALSO - it's been the same amount each time (4.2MB) - which is pretty interesting in itself.

=== NMT statistics fail

In this instance, printed NMT statistics from the REPL into the terminal output, but they were truncated by the scrollback limit, so I do not have the full values here. Have updated my debug REPL script to output to a file instead, so I can get the full output next time:

```
#!/bin/bash
export JDK_JAVA_OPTIONS="-Xmx3000m -Xms3000m -XX:MaxDirectMemorySize=3000m -XX:MaxMetaspaceSize=500m -XX:+UnlockDiagnosticVMOptions -XX:+PrintNMTStatistics -XX:NativeMemoryTracking=detail"
LOG_DIR="oom-investigation-notes/repl-logs"
LOG_FILE="$LOG_DIR/nmt-$(date +%Y%m%d-%H%M%S).log"
./gradlew :clojureRepl --bind 0.0.0.0 -PdebugJvm "$@" > "$LOG_FILE" 2>&1
```

I can also periodically run `jcmd <pid> VM.native_memory summary` to get a snapshot of the NMT stats at various point in time, so future runs will include that too.

== Thoughts from the above

I haven't particularly seen much direct evidence of a "leak" on cancelled queries as of the above - I am not entirely sure what the 4.2MB "jumps" in ambient netty usage are, but it is possible that this may relate to netty's own state management when handling data. 

I can say with relative confidence that it doesn't appear that EVERY cancelled/killed query leaves behind some state in netty, as I would expect to see a more consistent increase in the ambient netty memory usage if that were the case.

Given the correlation of "cancelled" error logs with memory spikes in netty as we have previously observed, it is still possible that there is some leaking going on here, though the exact conditions to trigger it are still unclear. 

Some potential areas of investigation:

* It may be that the leak only occurs when there are multiple simultaneous queries - perhaps multiple queries being cancelled at the same time on the same connection.
* Perhaps a leak occurs on an interrupt/cancellation during another error - ie, OutOfMemoryExceptions?
* It may be that a more complex/different query is required to trigger this - ie, the leak is only triggered in certain query plans,
** Some potential areas of interest there - other types of JOINs, GROUP BY, MATERIALIZED.
* The query may need to be cancelled at a particularly specific point in time - not unlike our previous memory leaks when an error occured when copying in direct memory.

In addition, I want to simplify my setup a bit:

* Would prefer to have the memory node setup & querying in the same file so I can run and test various things around connections from the REPL - this is currently a bit clunky with the psql connection, and it's hard to cancel things at specific points in time.
** Can look at pgwire_test for examples of pgwire usage.

== New test namespace

Rather than using dev.clj and pgwire, my new file ran at the REPL consists of the following:

```clojure
(ns memory-leak-investigation
  (:require [next.jdbc :as jdbc]
            [xtdb.node :as xtn])
  (:import [java.sql Connection]))

(def large-query
  "SELECT * FROM GENERATE_SERIES(1, 10000000) AS l(n) 
   LEFT JOIN GENERATE_SERIES(1, 10000000) 
   AS r(m) ON n = m 
   ORDER BY n
   LIMIT 100;")

(defn ->node []
  (xtn/start-node {:server {:port 5432
                            :host "*"}
                   :healthz {:port 8080
                             :host "*"}}))

(defn run-query [node]
  (let [connection-builder (.createConnectionBuilder node)]
    (with-open [^Connection conn (.build connection-builder)]
      (let [result (jdbc/execute! conn [large-query])]
        (println "First ten results of Query:" (take 10 result))))))

(defn run-and-cancel-query
  ([node] (run-and-cancel-query node 100))
  ([node sleep-ms]
   (let [connection-builder (.createConnectionBuilder node)]
     (with-open [^Connection conn (.build connection-builder)]
       (let [stmt (.createStatement conn)]
         (future
           (Thread/sleep sleep-ms)
           (.cancel stmt))
         (try
           (.executeQuery stmt large-query)
           (catch Exception e
             (println "Query was cancelled:" (.getMessage e)))))))))

(comment

  (def node (->node))

  (run-query node)
  
  (run-and-cancel-query node)

  (run-and-cancel-query node 1000)
  
  ;; when done/resetting
  
  (.close node)
  )
```

For the sake of comparison, I am using the same query as before - though can and will modify this query over time to see if I can trigger different behaviour / memory usage patterns.

== The 4MB Increments

Starting the node, I do a number of normal runs of the query, and see the following netty memory usage pattern:

image::clojure-normal-runs.png[]

Notes on this: 

* This graph shows the same query ran around 40 times in succession.
  * (This was originally intended to run 100 times, but I ran out of space on my device due to external sort)
* Over time, netty memory usage does increase by 4.2MB increments, even on normal runs, and this happens semi frequently.
* It then "settles" at a point, at which it will remain.

As an initial thought - we ARE using an in-memory node, and while we are not storing any data within tables, we will be storing transactions in the tx table AND in the in-memory log, so there is some state being kept in memory which grows over time. To verify this as the cause of the netty memory usage growth, I rewrote the node to be a "local" node:

```clojure
(defn ->node []
  (xtn/start-node {:storage [:local {:path "dev/oom-investigation/objects"}]
                   :log [:local {:path "dev/oom-investigation/log"} ]
                   :server {:port 5432
                            :host "*"}
                   :healthz {:port 8080
                             :host "*"}}))
```

Running the same set of 40 queries against the local node, I see the following netty memory usage pattern:

image::clojure-normal-runs-local.png[]

We still see this 4.2MB increment pattern, which would suggest that it isn't related to the in-memory tx table or log.

Also worth noting is the number of "live threads":

image::clojure-normal-runs-local-threads.png[] 

This climbs over time - one for each new connection - and stays there afterwards. 

Running ANOTHER set of 40 queries, I see the following for both netty memory usage and live threads:

image::clojure-normal-runs-local-2.png[]
image::clojure-normal-runs-local-threads-2.png[]

Notes on this:

* Seems like the ambient netty memory usage here doesn't go much further beyond 232MB.
* In fact, after this set of queries, it's even lower, sat at around 120MB.
* It seemingly doesn't increase forever, and neither does the live threads count. 
* Some form of caching _may_ be responsible here, such as prepared statements. 

Hard to say exactly what this is, but it does seem like the netty memory usage is related to something that grows over time, but has a limit. It seems unlikely to be related to any specific leak.

== OOMKilled Repro with TPCH

Following some investigation by Jeremy, we arrived at a potential repro.

Running XTDB within docker:

```
mkdir -p /tmp/xtdb1;
sudo chown -R 20000:20000 /tmp/xtdb1;
docker run -v /tmp/xtdb1:/var/lib/xtdb \
    --env XTDB_LOGGING_LEVEL='DEBUG' \
    --env JDK_JAVA_OPTIONS='-Xmx1000m -Xms1000m -XX:MaxDirectMemorySize=2000m -XX:MaxMetaspaceSize=256m' \
    -p 5432:5432 -p 8080:8080 \
    --memory="5000m" --memory-swap="5000m" \
    --pull=always ghcr.io/xtdb/xtdb:edge
```

And using the following from the REPL:
```
(ns memory-usage-tpch
  (:require [next.jdbc :as jdbc]
            [xtdb.datasets.tpch :as tpch]))

(defn ->connection []
  (jdbc/get-connection {:dbname "xtdb"
                        :host "localhost"
                        :port 5432
                        :classname "xtdb.jdbc.Driver"
                        :dbtype "xtdb"}))

;; submit TPCH SF 0.5
(with-open [conn (->connection)]
  (tpch/submit-dml-jdbc! conn 0.5))

;; followed by this query:
(with-open [conn (->connection)]
  (jdbc/execute! conn ["select count(*) from orders;"]))
```

Mid way through the TPCH submit, my docker container was OOMKilled (from docker ps --all):
```
9e8d9089a9b9   ghcr.io/xtdb/xtdb:edge "java -Dclojure.main…" 4 minutes ago   Exited (137)
```

Error code 137 is "killed" - ie, OOMKilled.

From `journalctl -k | grep -i oom`:
```
Memory cgroup out of memory: Killed process 22505 (java) total-vm:9079864kB, anon-rss:2799840kB, file-rss:25888kB, shmem-rss:0kB, UID:20000 pgtables:6252kB oom_score_adj:0
```

Some views of heap + direct memory usage graphs during the run:

image::tpch-oom-1.png[]
image::tpch-oom-2.png[]
image::tpch-oom-3.png[]

Assuming our overall memory usage of XTDB can be summarized by Heap usage and netty direct memory usage, we can see that we don't even really approach the 5GB limit of memory we set on the container. 

* The same is true even if we assume the FULL heap was being used in memory!

This would suggest that something ELSE is using a large amount of memory that isn't being tracked by these metrics.

One last thing to note - the entirety of the xtdb1 directory itself is around 2.3GB on disk:

```
2304172	xtdb1/
```

Next steps here:

* What happens if we remove memory-swap here?
* Try to gather some metrics from Docker itself to see if we can get a better idea of what memory is being used by the container.

=== Re-running without memory-swap

Re-running the above, but without memory-swap:

```
mkdir -p /tmp/xtdb1
sudo chown -R 20000:20000 /tmp/xtdb1
docker run -v /tmp/xtdb1:/var/lib/xtdb \
    --env XTDB_LOGGING_LEVEL='DEBUG' \
    --env JDK_JAVA_OPTIONS='-Xmx1000m -Xms1000m -XX:MaxDirectMemorySize=2000m -XX:MaxMetaspaceSize=256m' \
    -p 5432:5432 -p 8080:8080 \
    --memory="5000m" \
    --pull=always ghcr.io/xtdb/xtdb:edge
```
