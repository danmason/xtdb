= Memory Leak Investigation

This document shall focus on investigating memory leaking.

We shall start with https://github.com/xtdb/xtdb/issues/4757 - current running theory is that this relates to cancelled queries leaving behind some state in netty - presumably we need to be using DIRECT memory at the time, so need to force that in some way.

* Large JOIN queries seem to be the most likely culprit for direct memory usage - so we should try to reproduce with a large JOIN query, and then cancel it.

== Setup / Method

For this, will setup:

* Setting up a REPL with:
** A limited amount of JVM - `JDK_JAVA_OPTIONS="-Xmx3000m -Xms3000m -XX:MaxDirectMemorySize=3000m -XX:MaxMetaspaceSize=500m"`
** A number of memory leak tracking opts: `-XX:+UnlockDiagnosticVMOptions -XX:+PrintNMTStatistics -XX:NativeMemoryTracking=detail -Darrow.memory.debug.allocator=true -Dio.netty.leakDetection.level=paranoid`
* Run a memory only node - don't want anything pulled into memory cache to confuse things in terms of tracking netty memory usage.
** Added the following config to dev.clj
+
[src, clojure]
====
(def debug-config
  {::xtdb {:node-opts {:server {:port 5432
                                :host "*"}
                       :healthz {:port 8080
                                 :host "*"}}}})
(ir/set-prep! (fn [] debug-config))
====
* Setup prometheus & grafana from `monitoring` to track metrics - primarily Netty memory usage.

Then, to attempt to replicate the issue:

* Connect to the node with psql.
** `psql -h 0.0.0.0 -p 5432 -d xtdb`
* Run a query that should have a large direct memory usage, ie, a large LEFT JOIN query:
+
[src, sql]
====
SELECT * FROM GENERATE_SERIES(1, 10000000) AS l(n) LEFT JOIN GENERATE_SERIES(1, 10000000) AS r(m) ON n = m ORDER BY n LIMIT 100;
====
** Due to left join build ordering, should be quick on the heap memory side but should need to allocate a lot of direct memory for the output.
* I can then cancel the query from psql with Ctrl-C, if I want to.

== Initial observations

Initially, I just wanted to see how the direct memory usage looks over time as I run the above query to see if I can spot anything odd. I would expect:

* Some amount of direct memory usage as the query runs - this should be reflected in the netty memory usage metric.
* Following the query completing, the netty memory usage should drop back down to near zero, as the memory is released.

So, running the large generate series WITHOUT cancelling, I see the following (this is the results of a few different simultaneous runs):

image::normalrun.png[]

As we can see:

* Netty memory usage spikes to around 500MB during the query, which seems reasonable.
* It will remain at that value for a while as I fetch the results, but then it drops back down to near zero as expected.

All good so far - now, if we cancel a few of those queries (ie, I use CTRL+C on pgwire), we see the following (a side by side view over netty allocated memories and cancelled connection count):

image::cancelled-queries.png[]

Interestingly: 

* Netty memory usage does 'top off' at different points, depending on when I cancel the query.
* The usage does drop back down to near zero afterwards, which would suggest that there isn't a leak in this case.

To try something a bit more drastic, rather than cancelling the queries with CTRL+C, I instead just kill the psql process mid query. This results in the following:

image::killed-queries.png[]

So:

* Cancelled connections is, interestingly, unchanged.
* Again, netty memory usage goes up to around 500MB, and then drops back down to near zero.

However, looking a bit closer at values following one of these kills:

image::leftover-netty.png[]

Hard to see from the graph, but:

* Prior to the query, netty used memory sat at 8.4MB (this was true for all of the graphs above - not sure what I may have initially to have prompted that).
* AFTER the query - netty used memory sits at 12.6MB.

Now that isn't particularly drastic, but it does suggest that there may be some small amount of memory that isn't being released following a killed query, in certain cases.

* This didn't happen for BOTH of the killed queries I ran, only when I killed the query very quickly.
* Presumably will relate somewhat to "when" the thing was cancelled.

Following this, here's what happens when I run the query and send the cancel signal IMMEDIATELY after starting the query, a few times in quick succession:

image::quickly-cancelled.png[]

* After the first of these, my "base" value became 16.8MB.
* The following queries didn't share the same behaviour.

Still, it is interesting to see that the netty memory usage can increase in this way, even if it is only a small amount. ALSO - it's been the same amount each time (4.2MB) - which is pretty interesting in itself.

=== NMT statistics fail

In this instance, printed NMT statistics from the REPL into the terminal output, but they were truncated by the scrollback limit, so I do not have the full values here. Have updated my debug REPL script to output to a file instead, so I can get the full output next time:

```
#!/bin/bash
export JDK_JAVA_OPTIONS="-Xmx3000m -Xms3000m -XX:MaxDirectMemorySize=3000m -XX:MaxMetaspaceSize=500m -XX:+UnlockDiagnosticVMOptions -XX:+PrintNMTStatistics -XX:NativeMemoryTracking=detail"
LOG_DIR="oom-investigation-notes/repl-logs"
LOG_FILE="$LOG_DIR/nmt-$(date +%Y%m%d-%H%M%S).log"
./gradlew :clojureRepl --bind 0.0.0.0 -PdebugJvm "$@" > "$LOG_FILE" 2>&1
```

I can also periodically run `jcmd <pid> VM.native_memory summary` to get a snapshot of the NMT stats at various point in time, so future runs will include that too.

== Thoughts from the above

I haven't particularly seen much direct evidence of a "leak" on cancelled queries as of the above - I am not entirely sure what the 4.2MB "jumps" in ambient netty usage are, but it is possible that this may relate to netty's own state management when handling data. 

I can say with relative confidence that it doesn't appear that EVERY cancelled/killed query leaves behind some state in netty, as I would expect to see a more consistent increase in the ambient netty memory usage if that were the case.

Given the correlation of "cancelled" error logs with memory spikes in netty as we have previously observed, it is still possible that there is some leaking going on here, though the exact conditions to trigger it are still unclear. 

Some potential areas of investigation:

* It may be that the leak only occurs when there are multiple simultaneous queries - perhaps multiple queries being cancelled at the same time on the same connection.
* Perhaps a leak occurs on an interrupt/cancellation during another error - ie, OutOfMemoryExceptions?
* It may be that a more complex/different query is required to trigger this - ie, the leak is only triggered in certain query plans,
** Some potential areas of interest there - other types of JOINs, GROUP BY, MATERIALIZED.
* The query may need to be cancelled at a particularly specific point in time - not unlike our previous memory leaks when an error occured when copying in direct memory.

In addition, I want to simplify my setup a bit:

* Would prefer to have the memory node setup & querying in the same file so I can run and test various things around connections from the REPL - this is currently a bit clunky with the psql connection, and it's hard to cancel things at specific points in time.
** Can look at pgwire_test for examples of pgwire usage.

== New test namespace

Rather than using dev.clj and pgwire, my new file ran at the REPL consists of the following:

```clojure
(ns memory-leak-investigation
  (:require [next.jdbc :as jdbc]
            [xtdb.node :as xtn])
  (:import [java.sql Connection]))

(def large-query
  "SELECT * FROM GENERATE_SERIES(1, 10000000) AS l(n) 
   LEFT JOIN GENERATE_SERIES(1, 10000000) 
   AS r(m) ON n = m 
   ORDER BY n
   LIMIT 100;")

(defn ->node []
  (xtn/start-node {:server {:port 5432
                            :host "*"}
                   :healthz {:port 8080
                             :host "*"}}))

(defn run-query [node]
  (let [connection-builder (.createConnectionBuilder node)]
    (with-open [^Connection conn (.build connection-builder)]
      (let [result (jdbc/execute! conn [large-query])]
        (println "First ten results of Query:" (take 10 result))))))

(defn run-and-cancel-query
  ([node] (run-and-cancel-query node 100))
  ([node sleep-ms]
   (let [connection-builder (.createConnectionBuilder node)]
     (with-open [^Connection conn (.build connection-builder)]
       (let [stmt (.createStatement conn)]
         (future
           (Thread/sleep sleep-ms)
           (.cancel stmt))
         (try
           (.executeQuery stmt large-query)
           (catch Exception e
             (println "Query was cancelled:" (.getMessage e)))))))))

(comment

  (def node (->node))

  (run-query node)
  
  (run-and-cancel-query node)

  (run-and-cancel-query node 1000)
  
  ;; when done/resetting
  
  (.close node)
  )
```

For the sake of comparison, I am using the same query as before - though can and will modify this query over time to see if I can trigger different behaviour / memory usage patterns.

== The 4MB Increments

Starting the node, I do a number of normal runs of the query, and see the following netty memory usage pattern:

image::clojure-normal-runs.png[]

Notes on this: 

* This graph shows the same query ran around 40 times in succession.
  * (This was originally intended to run 100 times, but I ran out of space on my device due to external sort)
* Over time, netty memory usage does increase by 4.2MB increments, even on normal runs, and this happens semi frequently.
* It then "settles" at a point, at which it will remain.

As an initial thought - we ARE using an in-memory node, and while we are not storing any data within tables, we will be storing transactions in the tx table AND in the in-memory log, so there is some state being kept in memory which grows over time. To verify this as the cause of the netty memory usage growth, I rewrote the node to be a "local" node:

```clojure
(defn ->node []
  (xtn/start-node {:storage [:local {:path "dev/oom-investigation/objects"}]
                   :log [:local {:path "dev/oom-investigation/log"} ]
                   :server {:port 5432
                            :host "*"}
                   :healthz {:port 8080
                             :host "*"}}))
```

Running the same set of 40 queries against the local node, I see the following netty memory usage pattern:

image::clojure-normal-runs-local.png[]

We still see this 4.2MB increment pattern, which would suggest that it isn't related to the in-memory tx table or log.

Also worth noting is the number of "live threads":

image::clojure-normal-runs-local-threads.png[] 

This climbs over time - one for each new connection - and stays there afterwards. 

Running ANOTHER set of 40 queries, I see the following for both netty memory usage and live threads:

image::clojure-normal-runs-local-2.png[]
image::clojure-normal-runs-local-threads-2.png[]

Notes on this:

* Seems like the ambient netty memory usage here doesn't go much further beyond 232MB.
* In fact, after this set of queries, it's even lower, sat at around 120MB.
* It seemingly doesn't increase forever, and neither does the live threads count. 
* Some form of caching _may_ be responsible here, such as prepared statements. 

Hard to say exactly what this is, but it does seem like the netty memory usage is related to something that grows over time, but has a limit. It seems unlikely to be related to any specific leak.

== OOMKilled Repro with TPCH

Following some investigation by Jeremy, we arrived at a potential repro.

Running XTDB within docker:

```
mkdir -p /tmp/xtdb1;
sudo chown -R 20000:20000 /tmp/xtdb1;
docker run -v /tmp/xtdb1:/var/lib/xtdb \
    --env XTDB_LOGGING_LEVEL='DEBUG' \
    --env JDK_JAVA_OPTIONS='-Xmx1000m -Xms1000m -XX:MaxDirectMemorySize=2000m -XX:MaxMetaspaceSize=256m' \
    -p 5432:5432 -p 8080:8080 \
    --memory="5000m" --memory-swap="5000m" \
    --name="xtdb-oom-test" \
    --pull=always ghcr.io/xtdb/xtdb:edge 
```

And using the following from the REPL:
```
(ns memory-usage-tpch
  (:require [next.jdbc :as jdbc]
            [xtdb.datasets.tpch :as tpch]))

(defn ->connection []
  (jdbc/get-connection {:dbname "xtdb"
                        :host "localhost"
                        :port 5432
                        :classname "xtdb.jdbc.Driver"
                        :dbtype "xtdb"}))

;; submit TPCH SF 0.5
(with-open [conn (->connection)]
  (tpch/submit-dml-jdbc! conn 0.5))

;; followed by this query:
(with-open [conn (->connection)]
  (jdbc/execute! conn ["select count(*) from orders;"]))
```

Mid way through the TPCH submit, my docker container was OOMKilled (from docker ps --all):
```
9e8d9089a9b9   ghcr.io/xtdb/xtdb:edge "java -Dclojure.main…" 4 minutes ago   Exited (137)
```

Error code 137 is "killed" - ie, OOMKilled.

From `journalctl -k | grep -i oom`:
```
Memory cgroup out of memory: Killed process 22505 (java) total-vm:9079864kB, anon-rss:2799840kB, file-rss:25888kB, shmem-rss:0kB, UID:20000 pgtables:6252kB oom_score_adj:0
```

Some views of heap + direct memory usage graphs during the run:

image::tpch-oom-1.png[]
image::tpch-oom-2.png[]
image::tpch-oom-3.png[]

Assuming our overall memory usage of XTDB can be summarized by Heap usage and netty direct memory usage, we can see that we don't even really approach the 5GB limit of memory we set on the container. 

* The same is true even if we assume the FULL heap was being used in memory!

This would suggest that something ELSE is using a large amount of memory that isn't being tracked by these metrics.

One last thing to note - the entirety of the xtdb1 directory itself is around 2.3GB on disk:

```
2304172	xtdb1/
```

Next steps here:

* What happens if we remove memory-swap here?
* Try to gather some metrics from Docker itself to see if we can get a better idea of what memory is being used by the container.

== Re-running without memory-swap

Re-running the above, but without memory-swap:

```
mkdir -p /tmp/xtdb1
sudo chown -R 20000:20000 /tmp/xtdb1
docker run -v /tmp/xtdb1:/var/lib/xtdb \
    --env XTDB_LOGGING_LEVEL='DEBUG' \
    --env JDK_JAVA_OPTIONS='-Xmx1000m -Xms1000m -XX:MaxDirectMemorySize=2000m -XX:MaxMetaspaceSize=256m' \
    -p 5432:5432 -p 8080:8080 \
    --memory="5000m" \
    --name="xtdb-oom-test" \
    --pull=always ghcr.io/xtdb/xtdb:edge 
```

This ran for a _bit_ longer, but was ALSO OOMKilled - so memory-swap wasn't the issue.

== Capturing & observing docker metrics

=== Setup

For the sake of capturing docker cAdvisor metrics and displaying all of our relevant metrics, I have taken a copy of `monitoring` and modified it as such:

* Added `cadvisor` to `docker-compose.yml`
* Ensured `prometheus` is scraping cadvisor metrics
* Added an additional dashboard for all the memory related metrics - from our existing metrics and from cAdvisor.

This lives under `oom-investigation-notes/monitoring-stack`.

=== Running the repro

Running the above repro, and observing the memory usage over time, we see the following (side by side - cAdvisor memory usage on top, our existing memory metrics below):

image::metrics-with-cadvisor-1.png[]
image::metrics-with-cadvisor-2.png[]

So:

* We can see that the working set memory from cAdvisor DOES go above the set limit, which is what causes the OOMKill.
* The resident set is quite a bit lower than this, and stays below the limit.
* The working set size is far larger than both our netty memory usage and our heap usage combined.
* The memory cache sits at around 2.5GB at peak, which when added to the resident set, would account for the working set size. 

== The Working Set

Ultimately,`container_memory_working_set_bytes` is what is used by k8s for the sake of making OOM decisions, so this value going above our set limit is the cause of our OOMKills.

So, important for us to understand what this value is, and what it consists of.

=== What is it?

From the Kubernetes link:https://kubernetes.io/docs/reference/instrumentation/metrics/[*Kubernetes Metrics Reference*]:

> `container_memory_working_set_bytes`
>
> "Current working set of the container in bytes".

Expanding on their meaning of "working set", from link:https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#memory["Resource metrics pipeline"]:

> Memory is reported as the working set, measured in bytes, at the instant the metric was collected.
>
> In an ideal world, the "working set" is the amount of memory in-use that cannot be freed under memory pressure. However, calculation of the working set varies by host OS, and generally makes heavy use of heuristics to produce an estimate.
>
> The Kubernetes model for a container's working set expects that the container runtime counts anonymous memory associated with the container in question. The working set metric typically also includes some cached (file-backed) memory, because the host OS cannot always reclaim pages.

So, in summary, the working set is an estimate of the memory a container is actively using and that cannot be reclaimed under memory pressure.

It generally consists of:

* Anonymous memory (e.g. heap, stack)
* Some cached (file-backed) memory that the kernel cannot reclaim
* Other in-use memory (e.g. certain kernel allocations tied to the container)

It excludes:

* Free memory
* Easily reclaimable memory (e.g. parts of the page cache)

=== Comparing to Total

In our run, we see something quite interesting here when comparing the working set to the total memory usage from cAdvisor:

image::cadvisor-working-set.png[]
image::cadvisor-total.png[]

Specifically - the total memory usage is almost **identical** to our working set - ie, almost all of the memory being used by the container is considered to be "in use" and **not reclaimable**.

This seemingly includes (essentially) the entire container page cache, which is quite large - around 2.5GB at peak:

image::cadvisor-memory-cache.png[]

=== What the Page Cache Is

The page cache is RAM used by the Linux kernel to cache file-backed pages.
It sits between the process and the disk.

Contents:

* File data you've read (read() → kernel caches it)
* File data you've written (dirty pages waiting for flush)
* Memory-mapped files (mmap) — both read-only and read/write

A page in the cache is reclaimable if:

* It's clean (matches what's on disk) → Kernel can drop it instantly if memory pressure comes
* It's not pinned (i.e., no active mmap, DMA, or direct reference holding it)

==== When Pages Become Non-Reclaimable

Pages become "stuck" in the working set if:

* **Dirty but not flushed yet**
** Written to but not synced to disk
** Kernel must flush before reclaim
** If flushing is slow (disk IO bottleneck), dirty pages may pile up

* **Mapped into a process**
** If you mmap() a file (common in JVM world: JARs, RocksDB/Lucene indices, shared libs), the pages stay referenced
** They can still be evicted sometimes, but are harder to reclaim while mapped

* **mlock() or pinned by kernel/userspace**
** Explicitly locked into RAM → never reclaimable until unlocked

* **Cgroup memory accounting quirks**
** In Docker with memory limits, sometimes page cache is charged to the cgroup and appears non-reclaimable until eviction pressure hits

=== Potential Memory Double-Counting Theory

Haven't got a hard theory yet, but I am wondering if there's a problem with some leftover mmap stuff we're doing in XTDB - such as, for example, double counting memory in both the page cache and the allocator? 

Since we do some memory mapping and wrap foreign allocations in our RootAllocator, perhaps we are measuring the memory twice - once in the page cache and once in our allocator metrics. This could explain why so much of the page cache is non-reclaimable - because it's mapped into our process.

Alternatively - perhaps we are memory mapping and NOT counting it properly under the allocator, so any limits we are setting within the allocator are meaningless and not being respected.

I am specifically curious on the usage of mmap within the memory cache.

== Testing memory mapping usage

=== CLI tool

The `test_mmap.clj` program is a memory mapping testing utility that:

1. **Starts an XTDB node** to access the memory cache/buffer pool system
2. **Creates a byte buffer** of size 500MiB
3. **Stores the byte buffer** in the buffer pool using `putObject`
4. **Retrieves the byte buffer** using `getByteArray`, multiple times.
5. **Runs until manually stopped**, allowing observation of memory usage 

Code:
```clojure
(ns xtdb.test-mmap
  (:require [clojure.tools.logging :as log]
            [xtdb.node :as xtn]
            [xtdb.util :as util]
            [xtdb.db-catalog :as db-cat])
  (:import (java.nio ByteBuffer) 
           (java.util Random)
           (xtdb.database Database)))

(defn generate-random-byte-buffer ^ByteBuffer [buffer-size]
  (let [random (Random.)
        byte-buffer (ByteBuffer/allocate buffer-size)]
    (loop [i 0]
      (if (< i buffer-size)
        (do
          (.put byte-buffer (byte (.nextInt random 128)))
          (recur (inc i)))
        (.flip byte-buffer)))))

(defn create-500mb-buffer []
  (generate-random-byte-buffer (* 500 1024 1024)))

(defn run-test [node-opts] 
  (with-open [node (xtn/start-node node-opts)]
    (log/info "XTDB node started")

    (log/info "Waiting 1 minute for caches to warm up...")
    (Thread/sleep 60000)

    (let [^Database db (db-cat/primary-db node)
          buffer-pool (.getBufferPool db)
          test-path (util/->path "test-500mb.bin")
          test-buffer (create-500mb-buffer)]

      (log/info "Putting 500MB ByteBuffer into buffer pool...")
      (.putObject buffer-pool test-path test-buffer)
      (log/info "Successfully stored ByteBuffer")

      (log/info "Retrieving ByteBuffer from buffer pool...")
      (let [retrieved-data (.getByteArray buffer-pool test-path)]
        (log/info (format "Retrieved %d bytes" (count retrieved-data))))

      (Thread/sleep 10000)

      (log/info "Multiple retrievals from buffer pool...")
      (dotimes [i 5]
        (let [slice-data (.getByteArray buffer-pool test-path)]
          (log/info (format "Retrieval %d: %d bytes" (inc i) (count slice-data)))
          (Thread/sleep 10000)))

      ;; Keep running to observe memory patterns
      (try
        (while true
          (Thread/sleep 30000)
          (log/info "Still running - memory cache active"))
        (catch InterruptedException _
          (log/info "Test stopped"))))))
```

This is also added to the XTDB CLI under main.clj:

```
(def test-mmap-cli-spec
  [config-file-opt
   ["-h" "--help"]])

(defn- test-mmap [args]
  (let [{{:keys [file]} :options} (-> (parse-args args test-mmap-cli-spec)
                                      (handling-arg-errors-or-help))]
    (log/info "Starting memory mapping test...")
    ((requiring-resolve 'xtdb.test-mmap/run-test) (file->node-opts file))))
```

We build this as a docker image by building the local standalone docker image.

=== Running the test

For the dockerized XTDB image, I used the following command to run the `test_mmap.clj` program:

```bash
mkdir -p /tmp/xtdb1
sudo chown -R 20000:20000 /tmp/xtdb1
docker run -v /tmp/xtdb1:/var/lib/xtdb \
    --env XTDB_LOGGING_LEVEL='DEBUG' \
    --env JDK_JAVA_OPTIONS='-Xmx5000m -Xms5000m -XX:MaxDirectMemorySize=5000m -XX:MaxMetaspaceSize=256m' \
    -p 5432:5432 -p 8080:8080 \
    --name="xtdb-oom-test" docker.io/xtdb/xtdb test-mmap --file local_config.yaml
```

Ran alongside the monitoring stack to observe memory usage - I see the following memory metrics from cAdvisor and the XTDB node:

image::test-mmap-cadvisor.png[]
image::test-mmap-xtdb-metrics.png[]

So here's what we see, most importantly:

* We can see the point when when we load the 524MB (500MiB) buffer into the memory cache.
  * At that time - our netty memory/direct memory allocation here goes also to 524MB.
  * ALSO at that time - the page cache increases by around 524MB.
* The heap memory increases as we load the buffer with getByteArray - though eventually drops back down.
* Both the resident and working memory sets increase over time - the working memory set is and remains around 524MB LARGER than the resident set.

I believe this supports the theory that we are double counting memory here - the 500MiB buffer is being counted in both the netty direct memory usage AND in the page cache, which will leading to an overestimation of our actual memory usage by Kubernetes.

== Attempt at a fix

Assuming that the issue lies within memory_cache using unpooled buffers, we've made a change in there to grab the same netty allocator used by arrow (PooledByteBufAllocatorL), to ensure that we respect the netty memory limits on creating our buffers in the memory cache. We rebuilt the docker image and then re-ran TPCH against it, so to summarize we run:

```
mkdir -p /tmp/xtdb1
sudo chown -R 20000:20000 /tmp/xtdb1
docker run -v /tmp/xtdb1:/var/lib/xtdb \
    --env XTDB_LOGGING_LEVEL='DEBUG' \
    --env JDK_JAVA_OPTIONS='-Xmx1000m -Xms1000m -XX:MaxDirectMemorySize=2000m -XX:MaxMetaspaceSize=256m' \
    --memory="5000m" \
    -p 5432:5432 -p 8080:8080 \
    --name="xtdb-oom-test" docker.io/xtdb/xtdb
```

This still fails, and the netty memory usage is very similar to as it was before:

image::attempted-fix-memory-usage.png[]

=== Closing The FileChannel

Same as above, but also explicitly using .use() on the filechannel to ensure that also gets closed and doesn't permanently live in the page cache.

Once again, rebuilt the docker image and re-ran. Same result, essentially.

=== Setting memory reservation

Running through with the same changes, we wanted to see if we could "poke" the kernel into clearing up page cache, so we set a memory reservation on the docker container:

```
mkdir -p /tmp/xtdb1
sudo chown -R 20000:20000 /tmp/xtdb1
docker run -v /tmp/xtdb1:/var/lib/xtdb \
    --env XTDB_LOGGING_LEVEL='DEBUG' \
    --env JDK_JAVA_OPTIONS='-Xmx1000m -Xms1000m -XX:MaxDirectMemorySize=2000m -XX:MaxMetaspaceSize=256m' \
    --memory="5000m" \
    --memory-reservation="3000m" \
    -p 5432:5432 -p 8080:8080 \
    --name="xtdb-oom-test" docker.io/xtdb/xtdb
```

This similarly had essentially no effect - I also MANUALLY attempted to clear the page cache by running:

```
echo 3 | sudo tee -a /proc/sys/vm/drop_caches
```

This also had NO effect - the memory_cache values of the page cache remained unchanged.

=== Changes to main

Though the specifics to improve how we're handling memory within the memory_cache didn't have much of an impact on the overall memory usage/preventing OOMKills, I did want to ensure that we had the best possible handling of memory within the memory_cache, so I have merged these changes into main.

== Hypothesis on the Page Cache

My current thinking is that the page cache MUST be considered to be in use by unevictable pages - potentially that something holds open a file handle to the underlying files, which means that the pages cannot be evicted. As for what exactly is holding these files open, I am not sure - this could be something within netty, or arrow, or something with XTDB.

The TPCH process is dying entirely during ingest (currently) - so it's worth considering what may run/use memory during this process:

- The memory cache - caching files as bytebuffers we have loaded into memory locally.
- The compactor will spike in usage occasionally, though this shouldn't be consistently high.
- Indexer will open files and write memory out as it goes through.
- Query memory usage - we do some querying within execute-tx.
  - Query caches, also? Presumably all on heap.
- Anything being cached within pgwire? Connection state? Presumably, all on heap. 

== Testing agaist new main

Following current set of changes to introduce Disk Based Joins and move various implementations towards XTs arrow classes, decided to re-run TPCH to see if anything might have changed:

* Pulling the latest of main into this branch.
* Building the docker image.
* Running the docker image.
* Running the same TPCH repro.

So running this:

```
mkdir -p /tmp/xtdb1
sudo chown -R 20000:20000 /tmp/xtdb1
docker run -v /tmp/xtdb1:/var/lib/xtdb \
    --env XTDB_LOGGING_LEVEL='DEBUG' \
    --env JDK_JAVA_OPTIONS='-Xmx1000m -Xms1000m -XX:MaxDirectMemorySize=2000m -XX:MaxMetaspaceSize=256m' \
    --memory="5000m" \
    -p 5432:5432 -p 8080:8080 \
    --name="xtdb-oom-test" docker.io/xtdb/xtdb
```

Yet again - nearly identical in terms of memory usage patterns, and OOMKilled again.

A view over the whole memory usage here:

image::tpch-main-memory-usage.png[]

== Changing the memory cache limit

Currently the memory cache will attempt to use 50% of the direct memory set on the JVM, as it does by default. 

So, in this test we override the config on the docker container and set a lower explicit value in bytes for the memory cache, to see if this would have any impact on the overall memory usage/what is maintained in the page cache.

So, we build a new version of the standalone docker image, adding the following to `local_config.yaml`:

```yaml
...
memoryCache:
  maxSizeBytes: 536870912
```

Running with the same command as we did prior.

Immediately we can see the lower memory cache limit reflected in the XTDB memory metrics:

image::tpch-lower-mem-cache.png[]

Running TPCH against this - again, we see;

* A very similar memory usage pattern
* Far higher working set memory from cAdvisor than resident set memory - and high amount of page cache.
* Netty used memory is lower, as expected, due to the lower memory cache limit.
* Again, OOMKilled.

image:tpch-lower-mem-cache-usage.png[]

== Turning off compaction

It's not entirely out of the realm of possibility that there is some connection with compaction and open file handles, given that it is writing things down to disk and opening files to do so. In the process of running through ingestion, we run about 80 compaction jobs.

image::compactor-jobs-and-memory.png[]

To eliminate this area as one of concern, I removed the above memory cache limit, and instead disabled compaction entirely by adding the following to `local_config.yaml`:

```yaml
...
compactor:
  threads: 0
```  

Building the docker image and re-running TPCH against it, we observed that:

* This actually did prevent us getting an OOMKilled, at least during ingestion/subsequent query.
* The memory cache usage is basically zero UNTIL the query runs now.
* Subsequent queries fill up the memory cache.
* The total memory usage is still proportionally higher than the resident set, due to the large page cache.
* Didn't get an OOMKilled but sat at max memory usage of the container for a long time.

Following this have added this to our TPCH usage namespace to test more queries/fill up the memory cache:
```clojure
(with-open [conn (->connection)]
  (jdbc/execute! conn ["select count(*) from orders;"])
  (jdbc/execute! conn ["select count(*) from partsupp;"])
  (jdbc/execute! conn ["select count(*) from lineitem;"]))
```

=== Post run investigating

Given that the node was still running and 99% or so memory usage, decided to exec into the pod and look at some memory usage there, starting with `cat /proc/meminfo`: 

```
xtdb@e50eafce2d9a:/usr/local/lib/xtdb$ cat /sys/fs/cgroup/memory.stat
anon 2998587392
file 2040004608
kernel 178003968
kernel_stack 1277952
pagetables 7843840
sec_pagetables 0
percpu 7344
sock 0
vmalloc 32768
shmem 1986654208
zswap 161724525
zswapped 598487040
file_mapped 23355392
file_dirty 12288
file_writeback 0
swapcached 75448320
anon_thp 1275068416
file_thp 0
shmem_thp 0
inactive_anon 3517157376
active_anon 1491390464
inactive_file 49635328
active_file 3715072
unevictable 0
slab_reclaimable 5943072
slab_unreclaimable 1158928
slab 7102000
workingset_refault_anon 107992
workingset_refault_file 11602
workingset_activate_anon 105817
workingset_activate_file 4582
workingset_restore_anon 0
workingset_restore_file 23
workingset_nodereclaim 0
pgdemote_kswapd 0
pgdemote_direct 0
pgdemote_khugepaged 0
pgdemote_proactive 0
pgpromote_success 0
pgscan 412832
pgsteal 282953
pswpin 3347
pswpout 22808
pgscan_kswapd 0
pgscan_direct 412832
pgscan_khugepaged 0
pgscan_proactive 0
pgsteal_kswapd 0
pgsteal_direct 282953
pgsteal_khugepaged 0
pgsteal_proactive 0
pgfault 653547
pgmajfault 24159
pgrefill 38694
pgactivate 108653
pgdeactivate 0
pglazyfree 0
pglazyfreed 0
swpin_zero 258
swpout_zero 8008
zswpin 104387
zswpout 250548
zswpwb 0
thp_fault_alloc 1070
thp_collapse_alloc 0
thp_swpout 19
thp_swpout_fallback 0
numa_pages_migrated 0
numa_pte_updates 0
numa_hint_faults 0
```

Of particular note here:

* Unevictable is at  0 - so it doesn't seem like there are any pages that are locked in memory.

This does beg a few questions:

* Is this actually at zero with the compactor on, too? Was I just not seeing this due to not execing into the pod at the right time?
* Is this only at zero AFTER ingestion has finished, what kind of value does it have during ingestion? 
* Why is the total memory usage so high, if there are no unevictable pages?

Also, shmem is quite high - 1.9GB - which is "shared memory" - memory that may be simultaneously accessed by multiple processes with an intent to provide communication among them or avoid redundant copies.

We can limit this by setting `--shm-size` on the docker container, so this is something we can try next.

== Limiting shared memory

Back to running with compactor on, we set `--shm-size=512m` on the docker container to limit the amount of shared memory that can be used:

```
mkdir -p /tmp/xtdb1
sudo chown -R 20000:20000 /tmp/xtdb1
docker run -v /tmp/xtdb1:/var/lib/xtdb \
    --env XTDB_LOGGING_LEVEL='DEBUG' \
    --env JDK_JAVA_OPTIONS='-Xmx1000m -Xms1000m -XX:MaxDirectMemorySize=2000m -XX:MaxMetaspaceSize=256m' \
    --memory="5000m" \
    --shm-size="512m" \
    -p 5432:5432 -p 8080:8080 \
    --name="xtdb-oom-test" docker.io/xtdb/xtdb
```

We then ran TPCH against this - it OOMKilled, and the mid-run /sys/fs/cgroup/memory.stat was interesting too (picking some specific values here):
```
anon 2104250368
file 2203447296
kernel 923414528
kernel_stack 1523712
pagetables 7979008
vmalloc 32768
shmem 2063736832 
inactive_anon 1236643840
active_anon 2941202432
inactive_file 139694080
active_file 16384
unevictable 0
workingset_refault_anon 121174
workingset_refault_file 29342
workingset_activate_anon 85184
workingset_activate_file 10
workingset_restore_anon 125
workingset_restore_file 0
workingset_nodereclaim 0
``` 

shmem is still quite high, so it doesn't seem like limiting this has had much of an effect.

== Deeper memory profiling

Resetting us back to our original memory settings and adding some more detailed memory profiling to the JVM options, we have:

```
mkdir -p /tmp/xtdb1
sudo chown -R 20000:20000 /tmp/xtdb1
docker run -v /tmp/xtdb1:/var/lib/xtdb \
    --env XTDB_LOGGING_LEVEL='DEBUG' \
    --env JDK_JAVA_OPTIONS='-Xmx1000m -Xms1000m -XX:MaxDirectMemorySize=2000m -XX:MaxMetaspaceSize=256m -XX:+UnlockDiagnosticVMOptions -XX:+DebugNonSafepoints -XX:+HeapDumpOnOutOfMemoryError -XX:NativeMemoryTracking=summary' \
    --memory="5000m" \
    -p 5432:5432 -p 8080:8080 \
    --name="xtdb-oom-test" docker.io/xtdb/xtdb
```

I run TPCH against this, and then use a few commands against the pod to gather memory usage information:

* jcmd 1 VM.native_memory summary
* cat /proc/1/smaps

```
docker exec xtdb-oom-test cat /proc/1/smaps > smaps
docker exec xtdb-oom-test jcmd 1 VM.native_memory summary > nmt-summary
```

From these:

* NMT summary didn't reveal anything particularly interesting, equaling around the size of the resident set memory.

== Memory Reclaim Test

Given that the page cache is so high, I wanted to see if we could manually clear it out while the node was running, to see if this would have any effect on the overall memory usage or prevent OOMKills.

Running against our edge docker image:

```
mkdir -p /tmp/xtdb1
sudo chown -R 20000:20000 /tmp/xtdb1
docker run -v /tmp/xtdb1:/var/lib/xtdb \
    --env XTDB_LOGGING_LEVEL='DEBUG' \
    --env JDK_JAVA_OPTIONS='-Xmx1000m -Xms1000m -XX:MaxDirectMemorySize=2000m -XX:MaxMetaspaceSize=256m' \
    --memory="5000m" \
    -p 5432:5432 -p 8080:8080 \
    --name="xtdb-oom-test" \
    --privileged \
    --pull=always ghcr.io/xtdb/xtdb:edge 
```

We kick off TPCH as normal, and then when the total k8s memory begins to approach the limit, we exec into the pod and run:

```
docker exec xtdb-oom-test sh -c 'echo 2G > /sys/fs/cgroup/memory.reclaim'
```

I ran this a few times - it didn't seem to have MUCH of an effect, and we still hit OOMKills:

image::manual-clear-page-cache.png[]

I would have expected the page cache to drop down significantly - ie, by 2GB - but it didn't seem to have that much of an effect, and any drop in page cache was temporary, and it quickly climbed back up again.

We saw similar beforehand when trying to manually clear the page cache by writing to `/proc/sys/vm/drop_caches`.

I find it odd that I've seen unevictable pages at 0 when checking memory.stat, and yet the page cache remains so high and doesn't seem to ACT as if it can be cleared out.

== Working with cgroup

Wondering if potentially been looking at the wrong folder, I checked the cgroup directories on the host machine - from what we can see, the docker containers slice is under `/sys/fs/cgroup/system.slice/docker-<containerid>.scope` - there are MORE files under here than I saw on the pod itself, including `memory.high` - contents of this:

```
max
```

For the sake of testing, I ran the node and then set this to 4GB:

```
echo 4G | sudo tee /sys/fs/cgroup/system.slice/docker-d9c0def9bc811b165641141b87de6dcb13fc8a6a86745cbcfb41ad6d645bb13f.scope/memory.high
```

Doing TPCH again, we saw the following memory usage:

image::setting-memory-high.png[]

This again did NOT prevent an OOMKill, though we do see the memory usage actually hovers around 4GB for a lot of the time, so it does seem to be having some effect.

Seems like at the end it hits something that causes a spike in memory usage, which then OOMKills it  - what actually causes that to happen is a bit of a mystery, and we don't get much info from our node metrics when it DOES happen:

image::setting-memory-spike.png[]

=== Setting memory.high even lower

Same as above but setting `memory.high` even lower, to 3GB, we see the following memory usage:

image::setting-memory-high-3gb.png[]
